{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall and Fbeta\n",
    "\n",
    "To navigate the tricky trade-off between precision and recall ‚öñÔ∏è, we have the option to optimize F1, the harmonious mean of the two. However, in many scenarios, we need to prioritize one over another. \n",
    "\n",
    "For instance, in financial services, missing a fraudulent transaction can result in significant üí∞ financial losses and damage a company's reputation. To minimize such occurrences, optimizing recall (minimizing false negatives) is crucial, even if it means tolerating more false alarms. \n",
    "\n",
    "This is where `f_beta` score comes in handy üéØ. While still maintaining a balance between precision and recall, it allows us to fine-tune the importance of one over the other. When the beta value is greater than one, it places more emphasis on recall, while a beta value less than one emphasizes precision. For a simple demo on how f_beta works, please see [here](https://github.com/MenaWANG/ML_toy_examples/blob/main/train%20models/metrics-fbeta.ipynb).  \n",
    "\n",
    "Now, let's dive into a practical demo on how to fine-tune a LightGBM classifier to optimize Fbeta scores using {hyperopt}. üöÄüßê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6274165202108963 0.6263736263736264\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK, space_eval\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "from hyperopt.pyll.base import scope\n",
    "import hyperopt\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# demo data, a holdout set to evaluate the optimized model and avoid overfitting\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size = 0.2, random_state=123, stratify = y)\n",
    "\n",
    "print(y.mean(), y_train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define bata value and cv method\n",
    "beta = 2\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the space\n",
    "space = {\n",
    "    'hyper_params': {\n",
    "        'num_leaves': scope.int(hp.quniform('num_leaves', 10, 50, 1)),\n",
    "        'learning_rate': hp.loguniform('learning_rate', -5, 0),\n",
    "        'n_estimators': scope.int(hp.quniform('n_estimators', 50, 300, 1)),\n",
    "        'boosting_type': hp.choice('boosting_type', ['gbdt', 'dart', 'goss']),\n",
    "    },\n",
    "    'score_params': {\n",
    "        'threshold': hp.uniform('threshold', 0.0, 1.0),  # Specify threshold as a hyperparameter\n",
    "        'beta': hp.choice('beta', [beta])\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define the objective function for hyperparameter tuning and threshold optimization\n",
    "def objective(params):\n",
    "    hyper_params = params['hyper_params']    \n",
    "    threshold = params['score_params']['threshold']\n",
    "    beta = params['score_params']['beta']\n",
    "\n",
    "    clf = lgb.LGBMClassifier(**hyper_params,verbose=-1)\n",
    "    scores = []\n",
    "    for train_idx, valid_idx in cv.split(X, y):\n",
    "        X_train, X_valid = X[train_idx], X[valid_idx]\n",
    "        y_train, y_valid = y[train_idx], y[valid_idx]\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_prob = clf.predict_proba(X_valid)[:, 1]\n",
    "        # Apply the threshold to convert probabilities to predictions\n",
    "        y_pred = (y_pred_prob > threshold).astype(int)\n",
    "        # Calculate f score with the specified beta\n",
    "        score = fbeta_score(y_valid, y_pred, beta = beta)\n",
    "        scores.append(score)\n",
    "\n",
    "    # Calculate the mean F score\n",
    "    mean_score = np.mean(scores)\n",
    "\n",
    "    return {'loss': -mean_score, 'status': STATUS_OK}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [00:07<00:12,  4.98trial/s, best loss: -0.9849914493451487]\n",
      "Best hyperparameters: {'hyper_params': {'boosting_type': 'gbdt', 'learning_rate': 0.22796037589385654, 'n_estimators': 253, 'num_leaves': 38}, 'score_params': {'beta': 2, 'threshold': 0.28813841028148135}}\n"
     ]
    }
   ],
   "source": [
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            early_stop_fn=no_progress_loss(20),\n",
    "            trials=trials,\n",
    "            rstate=np.random.RandomState(42))\n",
    "\n",
    "best_params = space_eval(space, best)\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 score on the holdout set: 0.997\n"
     ]
    }
   ],
   "source": [
    "# performance on holdout set \n",
    "best_model = lgb.LGBMClassifier(**best_params['hyper_params'], verbose=-1)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_prob = best_model.predict_proba(X_holdout)\n",
    "y_pred = np.where(y_prob[:,1]>best_params['score_params']['threshold'], 1, 0)\n",
    "f2 = fbeta_score(y_holdout, y_pred, beta=2)\n",
    "print(f'F2 score on the holdout set: {f2:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting with `hp.quniform()`\n",
    "\n",
    "At first the code returned an error, complaining that `num_leaves` should be integer but received a float. See below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 109.0, 'num_leaves': 34.0}\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter search space using `hp.quniform() directly`\n",
    "space = {\n",
    "    'num_leaves': hp.quniform('num_leaves', 10, 50, 1),\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 300, 1),\n",
    "}\n",
    "print(hyperopt.pyll.stochastic.sample(space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpon search I found this relates to the known issue that `hp.quniform()` function not returning integers. A detailed report and discussions are available on the Hyperopt GitHub repository [here](https://github.com/hyperopt/hyperopt/issues/253).\n",
    "\n",
    "While waiting for the bug to be fixed, two temporary solutions were compared:\n",
    "\n",
    "A. Utilize `hp.choice()` as an alternative.    \n",
    "B. Manually cast the parameter values to integers. For example, `params['param'] = int(params['param'])`.\n",
    "\n",
    "In the first iteration of the project, I've opted for choice B, despite its slightly more verbose nature (pls see the next section). This approach ensures that the search values remain ordered, which can be particularly beneficial when using iterative search methods with `Hyperopt`.\n",
    "\n",
    "But then I found a more updated discussion on the same issue [here](https://github.com/hyperopt/hyperopt/issues/508), where I discovered a much neater solution (pls see below). I will check for related issues on github from now on, also I posted at the 1st thread about this, just in case anyone else stumbled upon it first like I did ü§™ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 55, 'num_leaves': 27}\n"
     ]
    }
   ],
   "source": [
    "# in short, the solution is `scope.int()`\n",
    "space = {\n",
    "    'num_leaves': scope.int(hp.quniform('num_leaves', 10, 50, 1)),\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 50, 300, 1)),\n",
    "}\n",
    "print(hyperopt.pyll.stochastic.sample(space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 1st iteration and the benefit of Iterative Development\n",
    "\n",
    "Below was the 1st iteration of the project. Three issues were fixed from there\n",
    "\n",
    "1. A less than ideal fix for `hp.quniform()` (see the section above)\n",
    "2. Used the default threshold (0.5) for determining positve/negative class \n",
    "3. Beta value can be better parameterized. \n",
    "\n",
    "The 1st iteration wasn't perfect, but it was quick and easy. In business context, a quick POC can be helpful in many ways. üòé Just to list a few:\n",
    "\n",
    "* **Rapid Validation (or Fail Fastü§™)**: POCs help validate the feasibility and viability of a machine learning project in a short timeframe. This prevents investing significant resources in an idea that might not work.\n",
    "* **Stakeholder Buy-Inüëè**: POCs provide a tangible result that you can show to stakeholders, making it easier to gain their support and secure budget for the full project.\n",
    "* **Baseline for Comparisonüìà**: POC performance provides a baseline against which you can compare the performance of the full-scale model. This helps in assessing the progress made and the actual impact of your machine learning solution.\n",
    "* **Iterative ImprovementüîÑ**: Results from a POC can be used to fine-tune and improve the full project. Lessons learned from a POC can guide you in making necessary adjustments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 44/100 [00:06<00:08,  6.37trial/s, best loss: -0.9860702979869748]\n",
      "Best hyperparameters: {'boosting_type': 'gbdt', 'learning_rate': 0.17935981742179688, 'n_estimators': 193, 'num_leaves': 13}\n",
      "F2 score on the holdout set: 0.986\n"
     ]
    }
   ],
   "source": [
    "# Define scorer for F2 and a cross-validation method\n",
    "def custom_f2_score(y_true, y_preb):\n",
    "    beta = 2 \n",
    "    return fbeta_score(y_true, y_preb, beta=beta)\n",
    "\n",
    "f_scorer = make_scorer(custom_f2_score, needs_proba=False)\n",
    "\n",
    "# Define the hyperparameter search space for the LightGBM classifier\n",
    "space = {\n",
    "    'num_leaves': scope.int(hp.quniform('num_leaves', 10, 50, 1)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, 0),\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 50, 300, 1)),\n",
    "    'boosting_type': hp.choice('boosting_type', ['gbdt', 'dart', 'goss']),\n",
    "}\n",
    "\n",
    "# Define the objective function for hyperparameter tuning\n",
    "def objective(params):\n",
    "    clf = lgb.LGBMClassifier(**params, verbose=-1)\n",
    "    score = cross_val_score(clf, X_train, y_train, cv=cv, scoring=f_scorer).mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Run optimization\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            early_stop_fn=no_progress_loss(20),\n",
    "            trials=trials,\n",
    "            rstate=np.random.RandomState(42))\n",
    "\n",
    "# Retrieve the best hyperparameters which returns the highest F2\n",
    "best_params = space_eval(space, best)\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# performance on holdout set \n",
    "best_model = lgb.LGBMClassifier(**best_params, verbose=-1)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_holdout)\n",
    "f2 = fbeta_score(y_holdout, y_pred, beta=2)\n",
    "print(f'F2 score on the holdout set: {f2:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38-modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
