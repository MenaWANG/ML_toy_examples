{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall and Fbeta\n",
    "\n",
    "To navigate the tricky trade-off between precision and recall ‚öñÔ∏è, we have the option to optimize F1, the harmonious mean of the two. However, in many scenarios, we need to prioritize one over another. \n",
    "\n",
    "For instance, in financial services, missing a fraudulent transaction can result in significant üí∞ financial losses and damage a company's reputation. To minimize such occurrences, optimizing recall (minimizing false negatives) is crucial, even if it means tolerating more false alarms. \n",
    "\n",
    "This is where `f_beta` score comes in handy üéØ. While still maintaining a balance between precision and recall, it allows us to fine-tune the importance of one over the other. When the beta value is greater than one, it places more emphasis on recall, while a beta value less than one emphasizes precision. For a simple demo on how f_beta works, please see [here](https://github.com/MenaWANG/ML_toy_examples/blob/main/train%20models/metrics-fbeta.ipynb).  \n",
    "\n",
    "Now, let's dive into a practical demo on how to fine-tune a LightGBM classifier to optimize Fbeta scores using {hyperopt}. üöÄüßê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6274165202108963 0.6263736263736264\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK, space_eval\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Step 2: Perform hyperparameter tuning using Hyperopt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size = 0.2, random_state=321, stratify = y)\n",
    "\n",
    "print(y.mean(), y_train.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune lightgbm classifier to optimize Fbeta (default threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [00:08<00:07,  6.25trial/s, best loss: -0.9838817508687244]\n",
      "Best hyperparameters: {'boosting_type': 'goss', 'learning_rate': 0.11365471176804709, 'n_estimators': 225, 'num_leaves': 43}\n"
     ]
    }
   ],
   "source": [
    "beta = 2\n",
    "# Define scorer for F2 and a cross-validation method\n",
    "def custom_f2_score(y_true, y_preb):\n",
    "    beta = beta \n",
    "    return fbeta_score(y_true, y_preb, beta=beta)\n",
    "f2_scorer = make_scorer(custom_f2_score, needs_proba=False)\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the objective function for hyperparameter tuning\n",
    "def objective(params):\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    clf = lgb.LGBMClassifier(**params, verbose=-1)\n",
    "    score = cross_val_score(clf, X_train, y_train, cv=cv, scoring=f2_scorer).mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Define the hyperparameter search space for the LightGBM classifier\n",
    "space = {\n",
    "    'num_leaves': hp.quniform('num_leaves', 10, 50, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, 0),\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 300, 1),\n",
    "    'boosting_type': hp.choice('boosting_type', ['gbdt', 'dart', 'goss']),\n",
    "}\n",
    "\n",
    "# Run optimization\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            early_stop_fn=no_progress_loss(20),\n",
    "            trials=trials)\n",
    "\n",
    "# Retrieve the best hyperparameters which returns the highest F2\n",
    "best_params = space_eval(space, best)\n",
    "best_params['num_leaves'] = int(best_params['num_leaves'])\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 score on the holdout set: 0.970\n"
     ]
    }
   ],
   "source": [
    "# performance on holdout set \n",
    "best_model = lgb.LGBMClassifier(**best_params, verbose=-1)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_holdout)\n",
    "f2 = fbeta_score(y_holdout, y_pred, beta=2)\n",
    "print(f'F2 score on the holdout set: {f2:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd Iteration: Optimize the threshold\n",
    "\n",
    "We've just completed our initial, minimalist iteration. Now let's make some further improvement at the second round. \n",
    "\n",
    "When calculating F_beta, we didn't specify the threshold used to distinguish between positive and negative cases. This led to the use of the default threshold, 0.5. However, this threshold should be fine-tuned as well. Let's do it in this second iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the space\n",
    "beta = 2\n",
    "space = {\n",
    "    'hyper_params': {\n",
    "        'num_leaves': hp.quniform('num_leaves', 10, 50, 1),\n",
    "        'learning_rate': hp.loguniform('learning_rate', -5, 0),\n",
    "        'n_estimators': hp.quniform('n_estimators', 50, 300, 1),\n",
    "        'boosting_type': hp.choice('boosting_type', ['gbdt', 'dart', 'goss'])\n",
    "    },\n",
    "    'score_params': {\n",
    "        'threshold': hp.uniform('threshold', 0.0, 1.0),  # Specify threshold as a hyperparameter\n",
    "        'beta': hp.choice('beta', [beta])\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define the objective function for hyperparameter tuning and threshold optimization\n",
    "def objective(params):\n",
    "    # should've been `hyper_params = params['hyper_params']` only, see discussions at the end\n",
    "    hyper_params = {\n",
    "        'num_leaves': int(params['hyper_params']['num_leaves']),\n",
    "        'learning_rate': params['hyper_params']['learning_rate'],\n",
    "        'n_estimators': int(params['hyper_params']['n_estimators']),\n",
    "        'boosting_type': params['hyper_params']['boosting_type']\n",
    "         }\n",
    "    \n",
    "    threshold = params['score_params']['threshold']\n",
    "    beta = params['score_params']['beta']\n",
    "\n",
    "    clf = lgb.LGBMClassifier(**hyper_params,verbose=-1)\n",
    "    scores = []\n",
    "    for train_idx, valid_idx in cv.split(X, y):\n",
    "        X_train, X_valid = X[train_idx], X[valid_idx]\n",
    "        y_train, y_valid = y[train_idx], y[valid_idx]\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_prob = clf.predict_proba(X_valid)[:, 1]\n",
    "        # Apply the threshold to convert probabilities to predictions\n",
    "        y_pred = (y_pred_prob > threshold).astype(int)\n",
    "        # Calculate f score with the specified beta\n",
    "        score = fbeta_score(y_valid, y_pred, beta = beta)\n",
    "        scores.append(score)\n",
    "\n",
    "    # Calculate the mean F score\n",
    "    mean_score = np.mean(scores)\n",
    "\n",
    "    return {'loss': -mean_score, 'status': STATUS_OK}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 74/100 [00:17<00:06,  4.23trial/s, best loss: -0.9860798668549601]\n",
      "Best hyperparameters: {'hyper_params': {'boosting_type': 'dart', 'learning_rate': 0.7781450728394775, 'n_estimators': 203, 'num_leaves': 27}, 'score_params': {'beta': 2, 'threshold': 0.3746420531186503}}\n"
     ]
    }
   ],
   "source": [
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            early_stop_fn=no_progress_loss(20),\n",
    "            trials=trials)\n",
    "\n",
    "best_params = space_eval(space, best)\n",
    "best_params['hyper_params']['num_leaves'] = int(best_params['hyper_params']['num_leaves'])\n",
    "best_params['hyper_params']['n_estimators'] = int(best_params['hyper_params']['n_estimators'])\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 score on the holdout set: 0.983\n"
     ]
    }
   ],
   "source": [
    "# performance on holdout set \n",
    "best_model = lgb.LGBMClassifier(**best_params['hyper_params'], verbose=-1)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_prob = best_model.predict_proba(X_holdout)\n",
    "y_pred = np.where(y_prob[:,1]>best_params['score_params']['threshold'], 1, 0)\n",
    "f2 = fbeta_score(y_holdout, y_pred, beta=2)\n",
    "print(f'F2 score on the holdout set: {f2:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussions: \n",
    "\n",
    "In the code above, you might notice some redundant lines that are a result of a known issue related to the `hp.quniform()`` function not returning integers. This issue has been reported and discussed in detail on the Hyperopt GitHub repository, and you can find the discussion [here](https://github.com/hyperopt/hyperopt/issues/253).\n",
    "\n",
    "While waiting for the bug to be fixed, there are at least two temporary solutions that can be applied:\n",
    "\n",
    "A. Utilize `hp.choice()`` as an alternative.\n",
    "B. Manually cast the parameter values to integers. For example, `params['param'] = int(params['param'])`.\n",
    "\n",
    "In the code provided, I've opted for choice B, despite its slightly more verbose nature. This approach ensures that the search values remain ordered, which can be particularly beneficial when using iterative search methods with `Hyperopt`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38-modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
