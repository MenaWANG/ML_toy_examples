{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested cross-validation \n",
    "\n",
    "Nested cross-validation üîé is a technique used in machine learning that serves the dual purpose of estimating the model's generalization error while simultaneously searching for the best set of hyperparameters or model configurations. It's a combination of two cross-validation loops: an outer loop and an inner loop.\n",
    "\n",
    "üîÑ Outer Cross-Validation (Outer Loop): The outer loop is responsible for estimating the model's performance. It typically uses k-fold cross-validation, where the original dataset is divided into k subsets or folds. In each iteration, one fold is used as a validation set, and the remaining k-1 folds are used for training. The model is trained and evaluated k times, and the average performance metric (e.g., accuracy, F1-score) is calculated over these iterations. This gives us an estimate of how well the model performs on unseen data.\n",
    "\n",
    "üîÑ Inner Cross-Validation (Inner Loop): Inside each iteration of the outer loop, there's another cross-validation loop. This inner loop is used for hyperparameter tuning or model selection. It's similar to the outer loop but focuses on selecting the best set of hyperparameters or model configurations. The inner loop also uses k-fold cross-validation but is applied to the training data from the outer loop. Different hyperparameter combinations or models are evaluated, and the best-performing combination is selected.\n",
    "\n",
    "![cross cv diagram](../.image/cross_cv.JPG)\n",
    "\n",
    "### key advantages of nested cross-validation\n",
    "\n",
    "- ‚úîÔ∏è Robust Performance Estimation: By using nested cross-validation, we obtain a more reliable estimate of our model's performance because it considers variations in both the training and validation data.\n",
    "\n",
    "- ‚úîÔ∏è Avoiding Data Leakage: Nested cross-validation helps prevent data leakage, which can occur when hyperparameter tuning or model selection is performed on the same data used for performance estimation. The inner loop ensures that model selection occurs on independent training and validation sets.\n",
    "\n",
    "- ‚úîÔ∏è Optimal Hyperparameter Tuning: It allows us to find the best hyperparameters or model configuration for our specific dataset while avoiding overfitting.\n",
    "\n",
    "### Example workflow in  cross-validation:\n",
    "\n",
    "**Outer Loop (Performance Estimation)**:\n",
    "\n",
    "- Split the dataset into k folds.\n",
    "- In each iteration:\n",
    "    - Use k-1 folds for training.\n",
    "    - Use the remaining fold for validation.\n",
    "    - Calculate a performance metric (e.g., accuracy) on the validation set.\n",
    "- Average the performance metrics from all iterations to estimate the model's overall performance.\n",
    "    \n",
    "**Inner Loop (Hyperparameter Tuning)**:\n",
    "\n",
    "- Inside each iteration of the outer loop:\n",
    "    - Split the training data from the outer loop into k folds.\n",
    "    - In each inner iteration:\n",
    "        - Use k-1 folds for training within the training data.\n",
    "        - Use the remaining fold for validation within the training data.\n",
    "        - Try different hyperparameter settings or model configurations.  \n",
    "        - Calculate a performance metric on the inner validation set.\n",
    "    - Choose the hyperparameters or model configuration that performed best on average across inner iterations.\n",
    "\n",
    "Nested cross-validation provides a more robust and unbiased way to evaluate and tune models, ensuring that our final model's performance estimates are more trustworthy. Here is a [great read](https://towardsdatascience.com/validating-your-machine-learning-model-25b4c8643fb7) on the topic, below pls see code demo to play with. üèÑüèª‚Äç‚ôÄÔ∏è "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, space_eval\n",
    "from sklearn.datasets import load_breast_cancer  \n",
    "from sklearn.ensemble import RandomForestClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Define the hyperparameter space to search\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', [int(x) for x in range(2, 11)]),\n",
    "    'min_samples_split': hp.uniform('min_samples_split', 0.1, 1.0),\n",
    "    'min_samples_leaf': hp.uniform('min_samples_leaf', 0.1, 0.5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:10<00:00,  1.07s/trial, best loss: -0.9783271874266948]\n",
      "test_auc in the outer loop 0.9902263374485596\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:11<00:00,  1.12s/trial, best loss: -0.9815660333098757]\n",
      "test_auc in the outer loop 0.9966196355085245\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:10<00:00,  1.07s/trial, best loss: -0.9816199859254047]\n",
      "test_auc in the outer loop 0.9952968841857731\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:10<00:00,  1.06s/trial, best loss: -0.9785322073657049]\n",
      "test_auc in the outer loop 0.9954438565549677\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:11<00:00,  1.11s/trial, best loss: -0.970885198217218]\n",
      "test_auc in the outer loop 0.9871399176954733\n",
      "Mean AUC: 0.993\n",
      "Standard Deviation: 0.0036\n"
     ]
    }
   ],
   "source": [
    "# Define the outer cross-validation loop\n",
    "outer_scores = []\n",
    "outer_loop_log = {}\n",
    "outer_cv = 5 # Number of outer loop iterations\n",
    "for i in range(outer_cv):  \n",
    "    # Split the data into training and test sets for the outer loop\n",
    "    X_train_outer, X_test_outer, y_train_outer, y_test_outer = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    def objective(params):\n",
    "        # Create a Random Forest classifier with the given hyperparameters\n",
    "        clf = RandomForestClassifier(**params)\n",
    "        # Use cross-validation to evaluate the model\n",
    "        scores = cross_val_score(clf, X_train_outer, y_train_outer, cv=5, scoring='roc_auc')\n",
    "        # Return the negative mean auc\n",
    "        return -np.mean(scores)\n",
    "    \n",
    "    # Optimize hyperparameters using Hyperopt (inner loop)\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "    best_params = space_eval(space, best)\n",
    "    outer_loop_log[f'fold{i}'] = {}\n",
    "    outer_loop_log[f'fold{i}']['best_params'] = best_params\n",
    "    outer_loop_log[f'fold{i}']['tune_auc'] = -trials.best_trial['result']['loss']\n",
    "    \n",
    "    # Create a Random Forest classifier with the best hyperparameters generated from the inner loop\n",
    "    clf = RandomForestClassifier(**best_params)\n",
    "    # Train the final model on the training data for the outer loop\n",
    "    clf.fit(X_train_outer, y_train_outer)\n",
    "    # Evaluate the final model on the test set for the outer loop\n",
    "    y_pred_outer = clf.predict_proba(X_test_outer)[:,1]\n",
    "    test_auc = roc_auc_score(y_test_outer, y_pred_outer)\n",
    "    outer_loop_log[f'fold{i}']['test_auc'] = test_auc\n",
    "    print('test_auc in the outer loop', test_auc)\n",
    "    outer_scores.append(test_auc)\n",
    "\n",
    "# Calculate the mean and standard deviation of outer loop scores\n",
    "mean_auc = np.mean(outer_scores)\n",
    "std_auc = np.std(outer_scores)\n",
    "\n",
    "print(\"Mean AUC: {:.3f}\".format(mean_auc))\n",
    "print(\"Standard Deviation: {:.4f}\".format(std_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above is reassuring since we have multiple sets of hyperparameters that all performed well. The very low standard deviation in performance across different outer loops suggests that the model is robust and not highly sensitive to the choice of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Best Fold and Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function finds the fold that performed the best on the selected metric (test_auc or tune_auc). We can then easily retrieve the best hyperparameters from the fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test Fold: fold1\n",
      "Best Test AUC: 0.9966196355085245\n",
      "Std Test AUC: 0.003643314302245934\n",
      "Hyperparameters for the best fold\n",
      " {'max_depth': 2, 'min_samples_leaf': 0.11518263089634831, 'min_samples_split': 0.17336266454096236}\n"
     ]
    }
   ],
   "source": [
    "def get_best_fold(outer_loop_log, metric):\n",
    "    \"\"\"This function find the fold that performed the best on a selected metric\n",
    "    \"\"\"\n",
    "    best_fold = None\n",
    "    best_score = 0.0  # Initialize with a lower value for AUC\n",
    "    metric_values = []  # Store metric values for all folds\n",
    "\n",
    "    for fold, data in outer_loop_log.items():\n",
    "        score = data[metric]\n",
    "        metric_values.append(score)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_fold = fold\n",
    "\n",
    "    std_metric = np.std(metric_values)\n",
    "\n",
    "    return best_fold, best_score, std_metric\n",
    "\n",
    "# Find the fold with the best performance and retrive its hyperparameters:\n",
    "best_test_fold, best_test_auc, std_test_auc = get_best_fold(outer_loop_log, 'test_auc')\n",
    "print(\"Best Test Fold:\", best_test_fold)\n",
    "print(\"Best Test AUC:\", best_test_auc)\n",
    "print(\"Std Test AUC:\", std_test_auc)\n",
    "print(\"Hyperparameters for the best fold\\n\", outer_loop_log[best_test_fold]['best_params'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38-modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
