{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested cross-validation \n",
    "\n",
    "Nested cross-validation is a technique used in machine learning to evaluate and tune models more robustly, especially when dealing with small datasets or when model performance is sensitive to variations in the training data. It's essentially a combination of two cross-validation loops: an outer loop and an inner loop.\n",
    "\n",
    "**Outer Cross-Validation (Outer Loop)**: This is the outer loop of the process and is responsible for estimating the model's performance. It typically uses k-fold cross-validation, where the original dataset is divided into k subsets or folds. In each iteration, one fold is used as a validation set, and the remaining k-1 folds are used for training. The model is trained and evaluated k times, and the average performance metric (e.g., accuracy, F1-score) is calculated over these iterations. This gives you an estimate of how well the model performs on unseen data.\n",
    "\n",
    "**Inner Cross-Validation (Inner Loop)**: Inside each iteration of the outer loop, there's another cross-validation loop. This inner loop is used for hyperparameter tuning or model selection. It's similar to the outer loop but focuses on selecting the best set of hyperparameters or model configuration. The inner loop also uses k-fold cross-validation but is applied to the training data from the outer loop. Different hyperparameter combinations or models are evaluated, and the best-performing combination is selected.\n",
    "\n",
    "### key advantages of nested cross-validation\n",
    "\n",
    "- Robust Performance Estimation: By using nested cross-validation, we obtain a more reliable estimate of our model's performance because it considers variations in both the training and validation data.\n",
    "\n",
    "- Avoiding Data Leakage: Nested cross-validation helps prevent data leakage, which can occur when hyperparameter tuning or model selection is performed on the same data used for performance estimation. The inner loop ensures that model selection occurs on independent training and validation sets.\n",
    "\n",
    "- Optimal Hyperparameter Tuning: It allows us to find the best hyperparameters or model configuration for our specific dataset while avoiding overfitting.\n",
    "\n",
    "### Example workflow in  cross-validation:\n",
    "\n",
    "**Outer Loop (Performance Estimation)**:\n",
    "\n",
    "- Split the dataset into k folds.\n",
    "- In each iteration:\n",
    "    - Use k-1 folds for training.\n",
    "    - Use the remaining fold for validation.\n",
    "    - Calculate a performance metric (e.g., accuracy) on the validation set.\n",
    "- Average the performance metrics from all iterations to estimate the model's overall performance.\n",
    "    \n",
    "**Inner Loop (Hyperparameter Tuning)**:\n",
    "\n",
    "- Inside each iteration of the outer loop:\n",
    "    - Split the training data from the outer loop into k folds.\n",
    "    - In each inner iteration:\n",
    "        - Use k-1 folds for training within the training data.\n",
    "        - Use the remaining fold for validation within the training data.\n",
    "        - Try different hyperparameter settings or model configurations.  \n",
    "        - Calculate a performance metric on the inner validation set.\n",
    "    - Choose the hyperparameters or model configuration that performed best on average across inner iterations.\n",
    "\n",
    "Nested cross-validation provides a more robust and unbiased way to evaluate and tune models, ensuring that our final model's performance estimates are more trustworthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, space_eval\n",
    "from hyperopt.pyll.base import scope\n",
    "from sklearn.datasets import load_breast_cancer  # Replace with your dataset\n",
    "from sklearn.ensemble import RandomForestClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Define the hyperparameter space to search\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', [int(x) for x in range(2, 11)]),\n",
    "    'min_samples_split': hp.uniform('min_samples_split', 0.1, 1.0),\n",
    "    'min_samples_leaf': hp.uniform('min_samples_leaf', 0.1, 0.5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:43<00:00,  1.16trial/s, best loss: -0.9840273047149894]\n",
      "best_params: {'max_depth': 2, 'min_samples_leaf': 0.13753606671347776, 'min_samples_split': 0.4045267970130479}\n",
      "test_auc in the outer loop 0.9917695473251029\n",
      "100%|██████████| 50/50 [00:40<00:00,  1.23trial/s, best loss: -0.9817827820783485]\n",
      "best_params: {'max_depth': 9, 'min_samples_leaf': 0.17861381870812446, 'min_samples_split': 0.4609184692006453}\n",
      "test_auc in the outer loop 0.9952968841857731\n",
      "100%|██████████| 50/50 [00:38<00:00,  1.31trial/s, best loss: -0.9828164203612479]\n",
      "best_params: {'max_depth': 6, 'min_samples_leaf': 0.11679396979203922, 'min_samples_split': 0.19794846916048284}\n",
      "test_auc in the outer loop 0.9972075249853026\n",
      "100%|██████████| 50/50 [00:55<00:00,  1.11s/trial, best loss: -0.9825460004691532]\n",
      "best_params: {'max_depth': 5, 'min_samples_leaf': 0.13198753586992315, 'min_samples_split': 0.37050727079099777}\n",
      "test_auc in the outer loop 0.9970605526161082\n",
      "100%|██████████| 50/50 [00:36<00:00,  1.39trial/s, best loss: -0.9831086089608257]\n",
      "best_params: {'max_depth': 8, 'min_samples_leaf': 0.1814528762707029, 'min_samples_split': 0.23817274107976089}\n",
      "test_auc in the outer loop 0.9945620223398001\n",
      "Mean AUC: 0.995\n",
      "Standard Deviation: 0.0020\n"
     ]
    }
   ],
   "source": [
    "# Define the outer cross-validation loop\n",
    "outer_scores = []\n",
    "outer_loop_log = {}\n",
    "for _ in range(5):  # Number of outer loop iterations\n",
    "    # Split the data into training and test sets for the outer loop\n",
    "    X_train_outer, X_test_outer, y_train_outer, y_test_outer = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    def objective(params):\n",
    "        # Create a Random Forest classifier with the given hyperparameters\n",
    "        clf = RandomForestClassifier(**params)\n",
    "        # Use cross-validation to evaluate the model\n",
    "        scores = cross_val_score(clf, X_train_outer, y_train_outer, cv=5, scoring='roc_auc')\n",
    "        # Return the negative mean accuracy (to maximize accuracy)\n",
    "        return -np.mean(scores)\n",
    "\n",
    "    # Optimize hyperparameters using Hyperopt (inner loop)\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "    best_params = space_eval(space, best)\n",
    "    print('best_params:', best_params)\n",
    "\n",
    "    # Create a Random Forest classifier with the best hyperparameters\n",
    "    clf = RandomForestClassifier(**best_params)\n",
    "    # Train the final model on the training data for the outer loop\n",
    "    clf.fit(X_train_outer, y_train_outer)\n",
    "    # Evaluate the final model on the test set for the outer loop\n",
    "    y_pred_outer = clf.predict_proba(X_test_outer)[:,1]\n",
    "    test_auc = roc_auc_score(y_test_outer, y_pred_outer)\n",
    "    print('test_auc in the outer loop', test_auc)\n",
    "    outer_scores.append(test_auc)\n",
    "\n",
    "# Calculate the mean and standard deviation of outer loop scores\n",
    "mean_auc = np.mean(outer_scores)\n",
    "std_auc = np.std(outer_scores)\n",
    "\n",
    "print(\"Mean AUC: {:.3f}\".format(mean_auc))\n",
    "print(\"Standard Deviation: {:.4f}\".format(std_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above is reassuring since we have multiple sets of hyperparameters that all perform. The very low standard deviation in performance across different outer loops suggests that the model is robust and not highly sensitive to the choice of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "set up an outer_loop_log dict\n",
    "* save outerloop test score, best_loss and best params\n",
    "* we can compare, sort by test score or best_loss and decide on which best params to use\n",
    "* easily calculate mean and std of test score across folds like we have now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38-modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
