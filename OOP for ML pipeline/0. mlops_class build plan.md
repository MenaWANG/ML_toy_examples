Plan and Progress

* 1st iteration
    * Preprocessing
        * ~~Build CustomTransformer, test and add to mlops_class~~
    * Model training and Optimization
        * ~~Build FeatureSelector, test and add to mlops_class~~
        * ~~Build NestedCVOptimizer, test and add to mlops_class~~
        * Create a HyperparamsTuner class (see 4 above)
        * Update NestedCVOptimizer class to use The HyperparamsTuner class as one of the optimizers
        * Create a Tuner class that utlize NextedCVOptimizer to optimize all there is to optimize, and return the best model
    * Model logging and management 
        * Use mlflow_pyfunc flavor to log the best model

* Further improvement
    * Simplify: 
        * More code sharing between two optimizers
        * Less configuration to tweak when setting up the Tuner
        * Same-name function shared by optimizers: both FeatureSelector and HyperparamsTuner are optimizers that can be called by NestedCVOptimizer, maybe we can have functions like optimizer.summary_plot() to show the optimizing progress.  
    * More functionalities
        * More algorithm available for FeatureSelector
        * In the above plan, we are running feature selection first then hyperparameter tunning next. Another option, is to identify the best feature for each hyperparams configuration, that is, add feature_selection process within the objective function for hyperparams tuning. The latter approach may give a more comprehensive search and capture interaction between hyperms setting and feature selections. But it is computationally more expensive. More importantly, it is also more prone to overfit if not handled carefully.
