# üìù Plan and Progress

## 1st iteration

* ‚úÖEDA functions
    * ‚úÖexploratory plots for numerical and categorical features 
    * ‚úÖscatterplots with highlight option 
* ‚úÖPreprocessing
    * ‚úÖBuild `CustomTransformer`, test and add to mlops_class
* Model training and Optimization
    * ‚úÖBuild `FeatureSelector`, test and add to mlops_class 
    * ‚úÖBuild `ModelOptimizer`, test and add to mlops_class 
    * ‚úÖCreate a `HyperparamsTuner` class (see 4 above) 
    * ‚úÖUpdate `ModelOptimizer` class to use `FeatureSelector` as one of the optimizers 
    * Less configuration to tweak when setting up the `ModelOptimizer`
        * ‚úÖe.g., Provide default cv and scorer 
    * üî≤Update `ModelOptimizer` class to use The `HyperparamsTuner` as one of the optimizers
    * üî≤Update `ModelOptimizer` class to utilize multiple optimizers and return the final best configuration. 
* Model logging and management 
    * ‚úÖBuild `ModelPipeline` class as the minimal object for final model consumption 
    * ‚úÖUpdate `ModelPipeline` to fit mlflow.pyfunc flavor
    * üî≤Update `ModelOptimizer` with a log function to train `ModelPipeline` with the final optimal configuration then log the best model


## Further improvement
    * Simplify: 
        * Similify model configuration passed on to ModelPipeline, maybe just the params, so the transformer and the model will be configured inside the class definition using the params passed in
        * More code sharing between two optimizers
        * Same-name function shared by optimizers: both FeatureSelector and HyperparamsTuner are optimizers that can be called by `ModelOptimizer`, maybe we can have functions like optimizer.summary_plot() to show the optimizing progress.  
    * More functionalities
        * More algorithm available for FeatureSelector
        * In the above plan, we are running feature selection first then hyperparameter tunning next. Another option, is to identify the best feature for each hyperparams configuration, that is, add feature_selection process within the objective function for hyperparams tuning. The latter approach may give a more comprehensive search and capture interaction between hyperms setting and feature selections. But it is computationally more expensive. More importantly, it is also more prone to overfit if not handled carefully.
