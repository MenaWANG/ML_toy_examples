Plan and Progress

* 1st iteration
    * EDA functions
        * ~~exploratory plots for numerical and categorical features~~
        * ~~scatterplots with highlight option~~
    * Preprocessing
        * ~~Build `CustomTransformer`, test and add to mlops_class~~
    * Model training and Optimization
        * ~~Build `FeatureSelector`, test and add to mlops_class~~
        * ~~Build `ModelOptimizer`, test and add to mlops_class~~
        * ~~Create a `HyperparamsTuner` class (see 4 above)~~
        * ~~Update `ModelOptimizer` class to use `FeatureSelector` as one of the optimizers~~
        * Less configuration to tweak when setting up the `ModelOptimizer`
            * ~~e.g., Provide default cv and scorer~~
        * Update `ModelOptimizer` class to use The `HyperparamsTuner` as one of the optimizers
        * Update `ModelOptimizer` class to utilize multiple optimizers and return the final best configuration. 
    * Model logging and management 
        * ~~Build `ModelPipeline` class as the minimal object for final model consumption~~
        * Update `ModelPipeline` to fit mlflow_pyfunc flavor 
        * Update `ModelOptimizer` with a log function to train `ModelPipeline` with the final optimal configuration then log the best model


* Further improvement
    * Simplify: 
        * More code sharing between two optimizers
        * Same-name function shared by optimizers: both FeatureSelector and HyperparamsTuner are optimizers that can be called by `ModelOptimizer`, maybe we can have functions like optimizer.summary_plot() to show the optimizing progress.  
    * More functionalities
        * More algorithm available for FeatureSelector
        * In the above plan, we are running feature selection first then hyperparameter tunning next. Another option, is to identify the best feature for each hyperparams configuration, that is, add feature_selection process within the objective function for hyperparams tuning. The latter approach may give a more comprehensive search and capture interaction between hyperms setting and feature selections. But it is computationally more expensive. More importantly, it is also more prone to overfit if not handled carefully.
