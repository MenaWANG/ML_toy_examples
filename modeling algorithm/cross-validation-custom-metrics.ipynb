{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook seeks to create cross validation of logistic regression with both built-in and custom metrics. \n",
    "\n",
    "This seemingly simple task turns out to be not that simple after all: For the case of `LogisticRegression` models, the `cross_validate()` function utilizes binary predictions rather than predicted probabilities for metrics calculation, which make the results inaccurate. \n",
    "\n",
    "There must be multiple solutions for the problem. The solution implemented in this notebook is to create a subclass of LogisticRegression() and enforce the `predict_proba()` returns there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, KFold, cross_val_predict\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, fbeta_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini scores: [0.95121951 0.94887266 0.89294489 0.93452381 0.81503268]\n",
      "AUC scores: [0.97560976 0.97443633 0.94647245 0.9672619  0.90751634]\n",
      "Mean Gini:        0.9085187104778096\n",
      "Mean AUC scores:  0.9542593552389048\n",
      "Predicts:         [[1.00000000e+00 2.24267433e-13]\n",
      " [9.99997384e-01 2.61599435e-06]]\n"
     ]
    }
   ],
   "source": [
    "# Load breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create logistic regression model\n",
    "model = LogisticRegression(max_iter = 5000)\n",
    "\n",
    "\n",
    "# Set up k-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "# Define scoring dictionary with multiple metrics\n",
    "# Define scoring dictionary with multiple metrics\n",
    "scoring = {\n",
    "    'gini': make_scorer(lambda y_true, y_pred_prob: \n",
    "                        2*roc_auc_score(y_true, y_pred_prob)-1),\n",
    "    'auc': make_scorer(lambda y_true, y_pred_prob: \n",
    "                        roc_auc_score(y_true, y_pred_prob))\n",
    "}\n",
    "# Perform cross-validation with multiple metrics\n",
    "scores = cross_validate(model, X, y, cv=kfold, scoring=scoring)\n",
    "\n",
    "predicts = cross_val_predict(model, X, y, cv=kfold, method = 'predict_proba')\n",
    "\n",
    "# Print the scores for each metric\n",
    "print(\"Gini scores:\", scores['test_gini'])\n",
    "print(\"AUC scores:\", scores['test_auc'])\n",
    "print(\"Mean Gini:       \", np.mean(scores['test_gini']))\n",
    "print(\"Mean AUC scores: \", np.mean(scores['test_auc']))\n",
    "print(\"Predicts:        \", predicts[:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many real-world business cases, we might want to utlize a customed metric to identify the optimal model for our own use case. It is very easy to implete with sklearn. \n",
    "\n",
    "Below is a (presumably) simple example where a function is created to calculate Fbeta score then feed into make_scorer() so it can be calculated in the cross-validation process. (for more on business cases, custom function and Fbeta, pls see Â·metrics-fbeta.iynb` in the same notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini scores: [0.95121951 0.94887266 0.89294489 0.93452381 0.81503268]\n",
      "AUC scores: [0.97560976 0.97443633 0.94647245 0.9672619  0.90751634]\n",
      "F_beta scores [0.98356885 0.97905952 0.96637335 0.97484493 0.92893924]\n",
      "Mean Gini:          0.9085187104778096\n",
      "Mean AUC score:     0.9542593552389048\n",
      "Mean F_beta score:  0.9665571768233491\n"
     ]
    }
   ],
   "source": [
    "def f_beta_prob(y_true, y_pred, threshold, beta):\n",
    "    \"\"\"\n",
    "    This function calculate Fbeta based on a threshold of 0.7, \n",
    "    which means cases with predicted probability higher than 0.7 \n",
    "    will be judged as positive by the model.\n",
    "    \"\"\"\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "    return fbeta_score(y_true, y_pred_binary, beta = beta)\n",
    "\n",
    "# Update the scoring dictionary with the new custom metrics\n",
    "scoring.update([ \n",
    "    ('f_beta', make_scorer(f_beta_prob, threshold=0.7, beta=0.8))\n",
    "])\n",
    "\n",
    "# Perform cross-validation with multiple metrics\n",
    "scores = cross_validate(model, X, y, cv=kfold, scoring=scoring)\n",
    "\n",
    "# Print the scores for each metric\n",
    "print(\"Gini scores:\", scores['test_gini'])\n",
    "print(\"AUC scores:\", scores['test_auc'])\n",
    "print('F_beta scores', scores['test_f_beta'])\n",
    "print(\"Mean Gini:         \", np.mean(scores['test_gini']))\n",
    "print(\"Mean AUC score:    \", np.mean(scores['test_auc']))\n",
    "print(\"Mean F_beta score: \", np.mean(scores['test_f_beta']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code seem straightforward and reasonable but actually the result is not accurate. Because as you can see in the printed result, the y_pred fed into the function is a binary judgement (based on a default threshold of 0.5) rather than a predicted probability. \n",
    "\n",
    "A related question would be: \"is the built-in function for roc_auc_score() receiving a predicted probability as it should?\" Coz if not, the cross-validated roc_auc_score and gini score printed above would be mis-leading as well! The following cell can confirm our suspicion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following cell to check the y_pred_prob which appear to be binary only\n",
    "scoring = {\n",
    "    'gini': make_scorer(lambda y_true, y_prob: (\n",
    "                            2*roc_auc_score(y_true, y_prob)-1,\n",
    "                            print(\"y_pred_prob values:\", y_prob)\n",
    "                        )),\n",
    "    'auc': make_scorer(lambda y_true, y_prob: (\n",
    "                            roc_auc_score(y_true, y_prob),\n",
    "                            print(\"y_pred_prob values:\", y_prob),\n",
    "                        ))\n",
    "}\n",
    "\n",
    "scores = cross_validate(model, X, y, cv=kfold, scoring=scoring)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a problem to solve: We need to feed `make_scorer()` function with the predicted probability to get the correct cross_validated metrics.  \n",
    "\n",
    "There must be multiple ways, but from the top of my head, at least we could create a new class\n",
    "* which is a subclass of LogisticRegression() and therefore inherit its abilities :P\n",
    "* then enforce this new class to return the predicted probilities (rather than binary predictions)\n",
    "\n",
    "Below pls see the code to impletement this simple idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini scores: [0.98128968 0.99301366 0.96489996 0.98941799 0.97908497]\n",
      "AUC scores: [0.99064484 0.99650683 0.98244998 0.99470899 0.98954248]\n",
      "f-beta: [0.98356885 0.97905952 0.95011265 0.95719844 0.94738713]\n",
      "Mean Gini (proba):            0.981541250535457\n",
      "Mean f-beta (proba):          0.9634653170418692\n",
      "Mean AUC scores (proba):      0.9907706252677286\n",
      "Mean AUC scores (original):   0.9542593552389048\n"
     ]
    }
   ],
   "source": [
    "class proba_logreg(LogisticRegression):\n",
    "    def __init__(self):\n",
    "        super().__init__(max_iter=5000)\n",
    "    def predict(self, X):\n",
    "        return LogisticRegression.predict_proba(self, X)\n",
    "    \n",
    "model_proba = proba_logreg()\n",
    "\n",
    "def f_beta_prob(y_true, y_prob, threshold, beta):\n",
    "    \"\"\"\n",
    "    This function calculate Fbeta based on a threshold of 0.7, \n",
    "    which means cases with predicted probability higher than 0.7 \n",
    "    will be judged as positive by the model.\n",
    "    \"\"\"\n",
    "    y_pred = (y_prob[:,1] >= threshold).astype(int)\n",
    "    return fbeta_score(y_true, y_pred, beta = beta)\n",
    "\n",
    "# Update the scoring dictionary with the new custom metrics\n",
    "scoring = {\n",
    "    'gini': make_scorer(lambda y_true, y_prob: \n",
    "                            2*roc_auc_score(y_true, y_prob[:,1])-1\n",
    "                        ),\n",
    "    'auc': make_scorer(lambda y_true, y_prob:\n",
    "                            roc_auc_score(y_true, y_prob[:,1])\n",
    "                        ),\n",
    "    'f_beta': make_scorer(lambda y_true, y_prob:\n",
    "                            f_beta_prob(y_true, y_prob, threshold=0.7, beta=0.8)\n",
    "                        )\n",
    "}\n",
    "\n",
    "scores_proba = cross_validate(model_proba, X, y, cv=kfold, scoring=scoring)\n",
    "\n",
    "print(\"Gini scores:\", scores_proba['test_gini'])\n",
    "print(\"AUC scores:\", scores_proba['test_auc'])\n",
    "print(\"f-beta:\", scores_proba['test_f_beta'])\n",
    "print(\"Mean Gini (proba):           \", np.mean(scores_proba['test_gini']))\n",
    "print(\"Mean f-beta (proba):         \", np.mean(scores_proba['test_f_beta']))\n",
    "print(\"Mean AUC scores (proba):     \", np.mean(scores_proba['test_auc']))\n",
    "print(\"Mean AUC scores (original):  \", np.mean(scores['test_auc']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
