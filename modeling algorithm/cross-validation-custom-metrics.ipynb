{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook seeks to create cross validation of logistic regression with both built-in and custom metrics. \n",
    "\n",
    "This seemingly simple task turns out to be not that simple after all: For the case of `LogisticRegression` models, the `cross_validate()` function utilizes binary predictions rather than predicted probabilities for metrics calculation, which make the results inaccurate. \n",
    "\n",
    "There must be multiple solutions for the problem. The solution implemented in this notebook is to create a subclass of LogisticRegression() and enforce the `predict_proba()` returns there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, KFold, cross_val_predict\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, fbeta_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini scores: [0.95121951 0.94887266 0.89294489 0.93452381 0.81503268]\n",
      "AUC scores: [0.97560976 0.97443633 0.94647245 0.9672619  0.90751634]\n",
      "Mean Gini:        0.9085187104778096\n",
      "Mean AUC scores:  0.9542593552389048\n",
      "Predicts:         [[1.00000000e+00 2.24267433e-13]\n",
      " [9.99997384e-01 2.61599435e-06]]\n"
     ]
    }
   ],
   "source": [
    "# Load breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create logistic regression model\n",
    "model = LogisticRegression(max_iter = 5000)\n",
    "\n",
    "\n",
    "# Set up k-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "# Define scoring dictionary with multiple metrics\n",
    "# Define scoring dictionary with multiple metrics\n",
    "scoring = {\n",
    "    'gini': make_scorer(lambda y_true, y_pred_prob: \n",
    "                        2*roc_auc_score(y_true, y_pred_prob)-1),\n",
    "    'auc': make_scorer(lambda y_true, y_pred_prob: \n",
    "                        roc_auc_score(y_true, y_pred_prob))\n",
    "}\n",
    "# Perform cross-validation with multiple metrics\n",
    "scores = cross_validate(model, X, y, cv=kfold, scoring=scoring)\n",
    "\n",
    "predicts = cross_val_predict(model, X, y, cv=kfold, method = 'predict_proba')\n",
    "\n",
    "# Print the scores for each metric\n",
    "print(\"Gini scores:\", scores['test_gini'])\n",
    "print(\"AUC scores:\", scores['test_auc'])\n",
    "print(\"Mean Gini:       \", np.mean(scores['test_gini']))\n",
    "print(\"Mean AUC scores: \", np.mean(scores['test_auc']))\n",
    "print(\"Predicts:        \", predicts[:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many real-world business cases, we might want to utlize a customed metric to identify the optimal model for our own use case. It is very easy to implete with sklearn. \n",
    "\n",
    "Below is a (presumably) simple example where a function is created to calculate Fbeta score then feed into make_scorer() so it can be calculated in the cross-validation process. (for more on business cases, custom function and Fbeta, pls see Â·metrics-fbeta.iynb` in the same notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred [0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0\n",
      " 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
      " 1 1 0]\n",
      "y_pred_binary [0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0\n",
      " 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
      " 1 1 0]\n",
      "y_pred [0 1 1 0 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0\n",
      " 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1\n",
      " 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1]\n",
      "y_pred_binary [0 1 1 0 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0\n",
      " 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1\n",
      " 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1]\n",
      "y_pred [0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1\n",
      " 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0\n",
      " 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1\n",
      " 1 1 0]\n",
      "y_pred_binary [0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1\n",
      " 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0\n",
      " 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1\n",
      " 1 1 0]\n",
      "y_pred [0 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0\n",
      " 0 0 1 0 1 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1\n",
      " 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1]\n",
      "y_pred_binary [0 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0\n",
      " 0 0 1 0 1 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1\n",
      " 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1]\n",
      "y_pred [0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1\n",
      " 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0\n",
      " 0 0]\n",
      "y_pred_binary [0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1\n",
      " 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0\n",
      " 0 0]\n",
      "Gini scores: [0.95121951 0.94887266 0.89294489 0.93452381 0.81503268]\n",
      "AUC scores: [0.97560976 0.97443633 0.94647245 0.9672619  0.90751634]\n",
      "F_beta scores [0.98356885 0.97905952 0.96637335 0.97484493 0.92893924]\n",
      "Mean Gini:          0.9085187104778096\n",
      "Mean AUC score:     0.9542593552389048\n",
      "Mean F_beta score:  0.9665571768233491\n"
     ]
    }
   ],
   "source": [
    "def f_beta_prob(y_true, y_pred, threshold, beta):\n",
    "    \"\"\"\n",
    "    This function calculate Fbeta based on a threshold of 0.7, \n",
    "    which means cases with predicted probability higher than 0.7 \n",
    "    will be judged as positive by the model.\n",
    "    \"\"\"\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "    print('y_pred', y_pred)\n",
    "    return fbeta_score(y_true, y_pred_binary, beta = beta)\n",
    "\n",
    "# Update the scoring dictionary with the new custom metrics\n",
    "scoring.update([ \n",
    "    ('f_beta', make_scorer(f_beta_prob, threshold=0.7, beta=0.8))\n",
    "])\n",
    "\n",
    "# Perform cross-validation with multiple metrics\n",
    "scores = cross_validate(model, X, y, cv=kfold, scoring=scoring)\n",
    "\n",
    "# Print the scores for each metric\n",
    "print(\"Gini scores:\", scores['test_gini'])\n",
    "print(\"AUC scores:\", scores['test_auc'])\n",
    "print('F_beta scores', scores['test_f_beta'])\n",
    "print(\"Mean Gini:         \", np.mean(scores['test_gini']))\n",
    "print(\"Mean AUC score:    \", np.mean(scores['test_auc']))\n",
    "print(\"Mean F_beta score: \", np.mean(scores['test_f_beta']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code seem straightforward and reasonable but actually the result is not accurate. Because as you can see in the printed result, the y_pred fed into the function is a binary judgement (based on a default threshold of 0.5) rather than a predicted probability. \n",
    "\n",
    "A related question would be: \"is the built-in function for roc_auc_score() receiving a predicted probability as it should?\" Coz if not, the cross-validated roc_auc_score and gini score printed above would be mis-leading as well! The following cell can confirm our suspicion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred_prob values: [0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0\n",
      " 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
      " 1 1 0]\n",
      "y_pred_prob values: [0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0\n",
      " 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
      " 1 1 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "scoring must return a number, got (0.9512195121951219, None) (<class 'tuple'>) instead. (scorer=gini)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Run the following cell to check the y_pred_prob\u001b[39;00m\n\u001b[0;32m      2\u001b[0m scoring \u001b[39m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mgini\u001b[39m\u001b[39m'\u001b[39m: make_scorer(\u001b[39mlambda\u001b[39;00m y_true, y_prob: (\n\u001b[0;32m      4\u001b[0m                             \u001b[39m2\u001b[39m\u001b[39m*\u001b[39mroc_auc_score(y_true, y_prob)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m                         ))\n\u001b[0;32m     11\u001b[0m }\n\u001b[1;32m---> 13\u001b[0m scores \u001b[39m=\u001b[39m cross_validate(model, X, y, cv\u001b[39m=\u001b[39;49mkfold, scoring\u001b[39m=\u001b[39;49mscoring)\n",
      "File \u001b[1;32mc:\\Users\\ningw\\anaconda3\\envs\\py38-modelling\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m    269\u001b[0m         X,\n\u001b[0;32m    270\u001b[0m         y,\n\u001b[0;32m    271\u001b[0m         scorers,\n\u001b[0;32m    272\u001b[0m         train,\n\u001b[0;32m    273\u001b[0m         test,\n\u001b[0;32m    274\u001b[0m         verbose,\n\u001b[0;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    276\u001b[0m         fit_params,\n\u001b[0;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ningw\\anaconda3\\envs\\py38-modelling\\lib\\site-packages\\joblib\\parallel.py:1048\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1040\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1048\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1049\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\ningw\\anaconda3\\envs\\py38-modelling\\lib\\site-packages\\joblib\\parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 864\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    865\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ningw\\anaconda3\\envs\\py38-modelling\\lib\\site-packages\\joblib\\parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    781\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 782\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    783\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    785\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\ningw\\anaconda3\\envs\\py38-modelling\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\ningw\\anaconda3\\envs\\py38-modelling\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\ningw\\anaconda3\\envs\\py38-modelling\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\ningw\\anaconda3\\envs\\py38-modelling\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\ningw\\anaconda3\\envs\\py38-modelling\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ningw\\anaconda3\\envs\\py38-modelling\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:708\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    705\u001b[0m result[\u001b[39m\"\u001b[39m\u001b[39mfit_error\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    707\u001b[0m fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m--> 708\u001b[0m test_scores \u001b[39m=\u001b[39m _score(estimator, X_test, y_test, scorer, error_score)\n\u001b[0;32m    709\u001b[0m score_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time \u001b[39m-\u001b[39m fit_time\n\u001b[0;32m    710\u001b[0m \u001b[39mif\u001b[39;00m return_train_score:\n",
      "File \u001b[1;32mc:\\Users\\ningw\\anaconda3\\envs\\py38-modelling\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:791\u001b[0m, in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[0;32m    789\u001b[0m                 score \u001b[39m=\u001b[39m score\u001b[39m.\u001b[39mitem()\n\u001b[0;32m    790\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(score, numbers\u001b[39m.\u001b[39mNumber):\n\u001b[1;32m--> 791\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg \u001b[39m%\u001b[39m (score, \u001b[39mtype\u001b[39m(score), name))\n\u001b[0;32m    792\u001b[0m         scores[name] \u001b[39m=\u001b[39m score\n\u001b[0;32m    793\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# scalar\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: scoring must return a number, got (0.9512195121951219, None) (<class 'tuple'>) instead. (scorer=gini)"
     ]
    }
   ],
   "source": [
    "# Run the following cell to check the y_pred_prob\n",
    "scoring = {\n",
    "    'gini': make_scorer(lambda y_true, y_prob: (\n",
    "                            2*roc_auc_score(y_true, y_prob)-1,\n",
    "                            print(\"y_pred_prob values:\", y_prob)\n",
    "                        )),\n",
    "    'auc': make_scorer(lambda y_true, y_prob: (\n",
    "                            roc_auc_score(y_true, y_prob),\n",
    "                            print(\"y_pred_prob values:\", y_prob),\n",
    "                        ))\n",
    "}\n",
    "\n",
    "scores = cross_validate(model, X, y, cv=kfold, scoring=scoring)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a problem to solve: We need to feed `make_scorer()` function with the predicted probability to get the correct cross_validated metrics.  \n",
    "\n",
    "There must be multiple ways, but from the top of my head, at least we could create a new class\n",
    "* which is a subclass of LogisticRegression() and therefore inherit its abilities :P\n",
    "* then enforce this new class to return the predicted probilities (rather than binary predictions)\n",
    "\n",
    "Below pls see the code to impletement this simple idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini scores: [0.98128968 0.99301366 0.96489996 0.98941799 0.97908497]\n",
      "AUC scores: [0.99064484 0.99650683 0.98244998 0.99470899 0.98954248]\n",
      "f-beta: [0.98356885 0.97905952 0.95011265 0.95719844 0.94738713]\n",
      "Mean Gini (proba):            0.981541250535457\n",
      "Mean f-beta (proba):          0.9634653170418692\n",
      "Mean AUC scores (proba):      0.9907706252677286\n",
      "Mean AUC scores (original):   0.9542593552389048\n"
     ]
    }
   ],
   "source": [
    "class proba_logreg(LogisticRegression):\n",
    "    def __init__(self):\n",
    "        super().__init__(max_iter=5000)\n",
    "    def predict(self, X):\n",
    "        return LogisticRegression.predict_proba(self, X)\n",
    "    \n",
    "model_proba = proba_logreg()\n",
    "\n",
    "def f_beta_prob(y_true, y_prob, threshold, beta):\n",
    "    \"\"\"\n",
    "    This function calculate Fbeta based on a threshold of 0.7, \n",
    "    which means cases with predicted probability higher than 0.7 \n",
    "    will be judged as positive by the model.\n",
    "    \"\"\"\n",
    "    y_pred = (y_prob[:,1] >= threshold).astype(int)\n",
    "    return fbeta_score(y_true, y_pred, beta = beta)\n",
    "\n",
    "# Update the scoring dictionary with the new custom metrics\n",
    "scoring = {\n",
    "    'gini': make_scorer(lambda y_true, y_prob: \n",
    "                            2*roc_auc_score(y_true, y_prob[:,1])-1\n",
    "                        ),\n",
    "    'auc': make_scorer(lambda y_true, y_prob:\n",
    "                            roc_auc_score(y_true, y_prob[:,1])\n",
    "                        ),\n",
    "    'f_beta': make_scorer(lambda y_true, y_prob:\n",
    "                            f_beta_prob(y_true, y_prob, threshold=0.7, beta=0.8)\n",
    "                        )\n",
    "}\n",
    "\n",
    "scores_proba = cross_validate(model_proba, X, y, cv=kfold, scoring=scoring)\n",
    "\n",
    "print(\"Gini scores:\", scores_proba['test_gini'])\n",
    "print(\"AUC scores:\", scores_proba['test_auc'])\n",
    "print(\"f-beta:\", scores_proba['test_f_beta'])\n",
    "print(\"Mean Gini (proba):           \", np.mean(scores_proba['test_gini']))\n",
    "print(\"Mean f-beta (proba):         \", np.mean(scores_proba['test_f_beta']))\n",
    "print(\"Mean AUC scores (proba):     \", np.mean(scores_proba['test_auc']))\n",
    "print(\"Mean AUC scores (original):  \", np.mean(scores['test_auc']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
