{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook seeks to create cross-validation of logistic regression with both built-in and custom metrics. \n",
    "\n",
    "This seemingly simple task turns out to be not that simple after all ðŸ¤ª: For the custom f_beta scorer, the `cross_validate()` function automatically feeds binary predictions rather than predicted probabilities into the scorer, which makes the results inaccurate. \n",
    "\n",
    "\n",
    "There must be multiple solutions to the problem. Three of them were tested and compared in this notebook. \n",
    "\n",
    "\n",
    "1. A bit hacky and lazy: To create a subclass of LogisticRegression() and enforce the `predict_proba()` returns there.\n",
    "\n",
    "2. Munual but ultimately customizable: Just create our own `cross_validate()` function. Then it can be customized to work with whatever self-defined estimators or processes we want to cross-validate. \n",
    "\n",
    "3. Define `needs_proba = True` in `make_scorer()` which is then fed into sklearn `cross_validate()`. Careful that even when we pass in `roc_auc_score()` to `make_scorer`, it won't automatically offer predicted probabilities if this is not specified (pls see details in the demo code and results below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, KFold, cross_val_predict, train_test_split\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, fbeta_score\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross_validate()\n",
    "\n",
    "Below are simple code doing cross-validation with LogisticRegression using two simple metrics, `roc_auc` and `gini`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini scores: [0.95121951 0.94887266 0.89294489 0.93452381 0.81503268]\n",
      "AUC scores: [0.97560976 0.97443633 0.94647245 0.9672619  0.90751634]\n",
      "Mean Gini:        0.9085187104778096\n",
      "Mean AUC scores:  0.9542593552389048\n",
      "Predicts:         [[1.00000000e+00 2.24267433e-13]\n",
      " [9.99997384e-01 2.61599435e-06]]\n"
     ]
    }
   ],
   "source": [
    "# Load breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create logistic regression model\n",
    "model = LogisticRegression(max_iter = 5000)\n",
    "\n",
    "# Set up k-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "# Define scoring dictionary with multiple metrics\n",
    "scoring = {\n",
    "    'auc':  make_scorer(lambda y_true, y_pred_prob: \n",
    "                        roc_auc_score(y_true, y_pred_prob)),\n",
    "    'gini': make_scorer(lambda y_true, y_pred_prob: \n",
    "                        2*roc_auc_score(y_true, y_pred_prob)-1),\n",
    "}\n",
    "# Perform cross-validation with multiple metrics\n",
    "scores = cross_validate(model, X, y, cv=kfold, scoring=scoring)\n",
    "\n",
    "predicts = cross_val_predict(model, X, y, cv=kfold, method = 'predict_proba')\n",
    "\n",
    "# Print the scores for each metric\n",
    "print(\"Gini scores:\", scores['test_gini'])\n",
    "print(\"AUC scores:\", scores['test_auc'])\n",
    "print(\"Mean Gini:       \", np.mean(scores['test_gini']))\n",
    "print(\"Mean AUC scores: \", np.mean(scores['test_auc']))\n",
    "print(\"Predicts:        \", predicts[:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a custom function\n",
    "In many real-world business cases, we might want to utlize a customed metric to identify the optimal model for our own use case. It is very easy to implete with sklearn. \n",
    "\n",
    "Below is a (presumably) simple example where a function is created to calculate Fbeta score then feed into make_scorer() so it can be calculated in the cross-validation process. (for more on business cases, custom function and Fbeta, pls see Â·metrics-fbeta.iynb` in the same notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini scores: [0.95 0.95 0.89 0.93 0.82]\n",
      "AUC scores: [0.98 0.97 0.95 0.97 0.91]\n",
      "F_beta scores: [0.98 0.98 0.97 0.97 0.93]\n",
      "Mean Gini:         0.91\n",
      "Mean AUC score:    0.95\n",
      "Mean F_beta score: 0.97\n"
     ]
    }
   ],
   "source": [
    "def f_beta_scorer(y_true, y_prob, threshold, beta):\n",
    "    \"\"\"\n",
    "    This function calculate Fbeta based on a threshold of 0.7, \n",
    "    which means cases with predicted probability higher than 0.7 \n",
    "    will be judged as positive by the model. \n",
    "    It also allow user to define beta value for f_beta score.\n",
    "\n",
    "    But the commented out code doesn't work, which means one-dimensional binary pred rather\n",
    "    than predicted probabilities was fed into the scorer\n",
    "    \"\"\"\n",
    "    #y_pred = (y_prob[:,1] >= threshold).astype(int)\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    return fbeta_score(y_true, y_pred, beta = beta)\n",
    "\n",
    "# Update the scoring dictionary with the new custom metrics\n",
    "scoring.update([ \n",
    "    ('f_beta', make_scorer(f_beta_scorer, threshold=0.7, beta=0.8))\n",
    "])\n",
    "\n",
    "# Perform cross-validation with multiple metrics\n",
    "scores = cross_validate(model, X, y, cv=kfold, scoring=scoring)\n",
    "\n",
    "# Print the scores for each metric\n",
    "print(\"Gini scores:\", np.round(scores['test_gini'], 2))\n",
    "print(\"AUC scores:\", np.round(scores['test_auc'], 2))\n",
    "print(\"F_beta scores:\", np.round(scores['test_f_beta'], 2))\n",
    "print(\"Mean Gini:         {:.2f}\".format(np.mean(scores['test_gini'])))\n",
    "print(\"Mean AUC score:    {:.2f}\".format(np.mean(scores['test_auc'])))\n",
    "print(\"Mean F_beta score: {:.2f}\".format(np.mean(scores['test_f_beta'])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem of the above calculations\n",
    "\n",
    "The above code seem straightforward and reasonable but actually the result is not accurate. Because as you can see from the comment in the `f_beta_scorer` function, binary y_pred (from `predict`) rather than probability predictions (from `predict_proba`) were fed into the scorer function.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1: Subclass and the revised `predict()`\n",
    "So now we have a problem to solve and there must be multiple ways to go around this. Below I will try two ways\n",
    "\n",
    "First, a sort of cheaky solution which poped into my head first is to create a new class (maybe due to or the OOP studies I did a while a go, see `OOP and multiple models.ipynb` also in this repo)\n",
    "* this new class is a subclass of LogisticRegression() and therefore inherit its abilities :P\n",
    "* then enforce this new class's `predict()` method to return the predicted probilities (rather than binary predictions). \n",
    "\n",
    "Below pls see the code to impletement this simple idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini scores: [0.98 0.99 0.96 0.99 0.98]\n",
      "AUC scores: [0.99 1.   0.98 0.99 0.99]\n",
      "f-beta: [0.98 0.98 0.95 0.96 0.95]\n",
      "Mean Gini (proba):           0.98\n",
      "Mean f-beta (proba):         0.96\n",
      "Mean AUC scores (proba):     0.99\n"
     ]
    }
   ],
   "source": [
    "class proba_logreg(LogisticRegression):\n",
    "    def __init__(self):\n",
    "        super().__init__(max_iter=5000)\n",
    "    def predict(self, X):\n",
    "        return LogisticRegression.predict_proba(self, X)\n",
    "    \n",
    "model_proba = proba_logreg()\n",
    "\n",
    "def f_beta_scorer(y_true, y_prob, threshold, beta):\n",
    "    \"\"\"\n",
    "    This function calculate Fbeta based on a threshold of 0.7, \n",
    "    which means cases with predicted probability higher than 0.7 \n",
    "    will be judged as positive by the model.\n",
    "    \"\"\"\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    return fbeta_score(y_true, y_pred, beta = beta)\n",
    "\n",
    "# Update the scoring dictionary with the new custom metrics\n",
    "scoring = {\n",
    "    'gini': make_scorer(lambda y_true, y_prob: \n",
    "                            2*roc_auc_score(y_true, y_prob[:,1])-1\n",
    "                        ),\n",
    "    'auc': make_scorer(lambda y_true, y_prob:\n",
    "                            roc_auc_score(y_true, y_prob[:,1])\n",
    "                        ),\n",
    "    'f_beta': make_scorer(lambda y_true, y_prob:\n",
    "                            f_beta_scorer(y_true, y_prob[:,1], threshold=0.7, beta=0.8)\n",
    "                        )\n",
    "}\n",
    "\n",
    "scores_proba = cross_validate(model_proba, X, y, cv=kfold, scoring=scoring)\n",
    "\n",
    "print(\"Gini scores:\", np.round(scores_proba['test_gini'],2))\n",
    "print(\"AUC scores:\", np.round(scores_proba['test_auc'],2))\n",
    "print(\"f-beta:\", np.round(scores_proba['test_f_beta'],2))\n",
    "print(\"Mean Gini (proba):           {:.2f}\".format(np.mean(scores_proba['test_gini'])))\n",
    "print(\"Mean f-beta (proba):         {:.2f}\".format(np.mean(scores_proba['test_f_beta'])))\n",
    "print(\"Mean AUC scores (proba):     {:.2f}\".format(np.mean(scores_proba['test_auc'])))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: Do it manually\n",
    "\n",
    "Of course, we could simply do the splits and calculate the relevant metrics manually like below. \n",
    "\n",
    "The manual process comes with the benefit of ultimate customizability:\n",
    "\n",
    "1. it can be used to cross-validate any sklearn or customed algorithm class. (The first approach above is able to do this as well.) \n",
    "2. it can be easily applied from any stage of the modeling process. For example, if I have a customed class that does both feature engineering and model fit, using this approach I could either just test the fitted model acrossed kfold data or test the combined process by doing the kfold splits before feature engineering. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GINI: Mean = 0.99, Std Dev = 0.01\n",
      "AUC: Mean = 0.99, Std Dev = 0.01\n",
      "F_BETA: Mean = 0.96, Std Dev = 0.01\n"
     ]
    }
   ],
   "source": [
    "# Set the number of folds\n",
    "k = 5\n",
    "\n",
    "# Split the data into k folds\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "# Initialize an array to store the results\n",
    "results = {'gini': np.zeros(k), 'auc': np.zeros(k), 'f_beta': np.zeros(k)}\n",
    "\n",
    "# Loop over the folds\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    \n",
    "    # Get the training and testing data for this fold\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    model.fit(X_train, y_train)    \n",
    "    \n",
    "    # Evaluate the model on the testing data and store the results\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    results['gini'][i] = 2*roc_auc_score(y_test, y_prob) - 1\n",
    "    results['auc'][i] = roc_auc_score(y_test, y_prob)\n",
    "    results['f_beta'][i] = f_beta_scorer(y_test, y_prob, threshold=0.7, beta=0.8)\n",
    "\n",
    "# Compute the mean and standard deviation of the results for each metric\n",
    "mean_results = {metric: np.mean(results[metric]) for metric in results}\n",
    "std_devs = {metric: np.std(results[metric]) for metric in results}\n",
    "\n",
    "# Print the results\n",
    "for metric in results:\n",
    "    print(\"{}: Mean = {:.2f}, Std Dev = {:.2f}\".format(metric.upper(), \n",
    "                                                        mean_results[metric], \n",
    "                                                        std_devs[metric]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3 and Discussions\n",
    "\n",
    "As you probably noticed, the gini and auc scores from the two solutions seem much higher than what we received first. Therefore I suspect that when we use `make_scorer`, even when calculating auc score with the sklearn built in function `roc_auc_score`, `predict()` is the default input into our scorer. If that's the case, is there a way to adjust? Yes, all we need to do is to add `needs_proba = True` as an argument for the `make_scorer` function.\n",
    "\n",
    "Upon studying the doc, I also found that for some built-in functions like `roc_auc_score`, we could pass its identifier into the scoring dictionary. For example, in the code below we could say `{'auto auc': 'roc_auc'}`, sk-learn recognizes `roc_auc` as an identifier for the roc_auc_score metric. It will automatically handle the selection of `predict_proba` to obtain the predicted probabilities for calculating the ROC AUC score. But this is not much use when we have more complex custom scorers. \n",
    "\n",
    "To summarize, if we only need to cross-validate an estimator's fit on certain metrics, either solution 1 or 3 will suffice. Solution 3 is the standard approach, while solution 1 might be useful for a slightly trickier task.\n",
    "\n",
    "Last but not least, if we desire the utmost customization of the cross-validation process, solution 2 is likely the way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini scores: [0.98128968 0.99301366 0.96489996 0.98941799 0.97908497]\n",
      "AUC scores: [0.99064484 0.99650683 0.98244998 0.99470899 0.98954248]\n",
      "auto AUC scores: [0.99064484 0.99650683 0.98244998 0.99470899 0.98954248]\n",
      "Mean Gini:        0.981541250535457\n",
      "Mean AUC scores:  0.9907706252677286\n",
      "Mean auto AUC scores:  0.9907706252677286\n"
     ]
    }
   ],
   "source": [
    "# Load breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create logistic regression model\n",
    "model = LogisticRegression(max_iter = 5000)\n",
    "\n",
    "# Set up k-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "# Define scoring dictionary with multiple metrics\n",
    "scoring = {\n",
    "    'auc':  make_scorer(lambda y_true, y_pred_prob: \n",
    "                        roc_auc_score(y_true, y_pred_prob), needs_proba = True),\n",
    "    'gini': make_scorer(lambda y_true, y_pred_prob: \n",
    "                        2*roc_auc_score(y_true, y_pred_prob)-1, needs_proba = True),\n",
    "    'auc_auto': 'roc_auc'\n",
    "}\n",
    "# Perform cross-validation with multiple metrics\n",
    "scores = cross_validate(model, X, y, cv=kfold, scoring=scoring)\n",
    "\n",
    "#Print the scores for each metric\n",
    "print(\"Gini scores:\", scores['test_gini'])\n",
    "print(\"AUC scores:\", scores['test_auc'])\n",
    "print(\"auto AUC scores:\", scores['test_auc_auto'])\n",
    "print(\"Mean Gini:       \", np.mean(scores['test_gini']))\n",
    "print(\"Mean AUC scores: \", np.mean(scores['test_auc']))\n",
    "print(\"Mean auto AUC scores: \", np.mean(scores['test_auc_auto']))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
