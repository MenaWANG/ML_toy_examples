{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook seeks to create cross validation of logistic regression with both built-in and custom metrics. \n",
    "\n",
    "This seemingly simple task turns out to be not that simple after all: For the case of `LogisticRegression` models, the `cross_validate()` function utilizes binary predictions rather than predicted probabilities for metrics calculation, which make the results inaccurate. \n",
    "\n",
    "There must be multiple solutions for the problem. The solution implemented in this notebook is to create a subclass of LogisticRegression() and enforce the `predict_proba()` returns there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, KFold, cross_val_predict, train_test_split\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, fbeta_score\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini scores: [0.95121951 0.94887266 0.89294489 0.93452381 0.81503268]\n",
      "AUC scores: [0.97560976 0.97443633 0.94647245 0.9672619  0.90751634]\n",
      "Mean Gini:        0.9085187104778096\n",
      "Mean AUC scores:  0.9542593552389048\n",
      "Predicts:         [[1.00000000e+00 2.24267433e-13]\n",
      " [9.99997384e-01 2.61599435e-06]]\n"
     ]
    }
   ],
   "source": [
    "# Load breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create logistic regression model\n",
    "model = LogisticRegression(max_iter = 5000)\n",
    "\n",
    "\n",
    "# Set up k-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "# Define scoring dictionary with multiple metrics\n",
    "# Define scoring dictionary with multiple metrics\n",
    "scoring = {\n",
    "    'gini': make_scorer(lambda y_true, y_pred_prob: \n",
    "                        2*roc_auc_score(y_true, y_pred_prob)-1),\n",
    "    'auc': make_scorer(lambda y_true, y_pred_prob: \n",
    "                        roc_auc_score(y_true, y_pred_prob))\n",
    "}\n",
    "# Perform cross-validation with multiple metrics\n",
    "scores = cross_validate(model, X, y, cv=kfold, scoring=scoring)\n",
    "\n",
    "predicts = cross_val_predict(model, X, y, cv=kfold, method = 'predict_proba')\n",
    "\n",
    "# Print the scores for each metric\n",
    "print(\"Gini scores:\", scores['test_gini'])\n",
    "print(\"AUC scores:\", scores['test_auc'])\n",
    "print(\"Mean Gini:       \", np.mean(scores['test_gini']))\n",
    "print(\"Mean AUC scores: \", np.mean(scores['test_auc']))\n",
    "print(\"Predicts:        \", predicts[:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a custom function\n",
    "In many real-world business cases, we might want to utlize a customed metric to identify the optimal model for our own use case. It is very easy to implete with sklearn. \n",
    "\n",
    "Below is a (presumably) simple example where a function is created to calculate Fbeta score then feed into make_scorer() so it can be calculated in the cross-validation process. (for more on business cases, custom function and Fbeta, pls see Â·metrics-fbeta.iynb` in the same notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini scores: [0.95 0.95 0.89 0.93 0.82]\n",
      "AUC scores: [0.98 0.97 0.95 0.97 0.91]\n",
      "F_beta scores: [0.98 0.98 0.97 0.97 0.93]\n",
      "Mean Gini:         0.91\n",
      "Mean AUC score:    0.95\n",
      "Mean F_beta score: 0.97\n"
     ]
    }
   ],
   "source": [
    "def f_beta_prob(y_true, y_pred, threshold, beta):\n",
    "    \"\"\"\n",
    "    This function calculate Fbeta based on a threshold of 0.7, \n",
    "    which means cases with predicted probability higher than 0.7 \n",
    "    will be judged as positive by the model.\n",
    "    \"\"\"\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "    return fbeta_score(y_true, y_pred_binary, beta = beta)\n",
    "\n",
    "# Update the scoring dictionary with the new custom metrics\n",
    "scoring.update([ \n",
    "    ('f_beta', make_scorer(f_beta_prob, threshold=0.7, beta=0.8))\n",
    "])\n",
    "\n",
    "# Perform cross-validation with multiple metrics\n",
    "scores = cross_validate(model, X, y, cv=kfold, scoring=scoring)\n",
    "\n",
    "# Print the scores for each metric\n",
    "print(\"Gini scores:\", np.round(scores['test_gini'], 2))\n",
    "print(\"AUC scores:\", np.round(scores['test_auc'], 2))\n",
    "print(\"F_beta scores:\", np.round(scores['test_f_beta'], 2))\n",
    "print(\"Mean Gini:         {:.2f}\".format(np.mean(scores['test_gini'])))\n",
    "print(\"Mean AUC score:    {:.2f}\".format(np.mean(scores['test_auc'])))\n",
    "print(\"Mean F_beta score: {:.2f}\".format(np.mean(scores['test_f_beta'])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem of the above calculations\n",
    "\n",
    "The above code seem straightforward and reasonable but actually the result is not accurate. Because as you can see in the printed result, the y_pred fed into the function is a binary judgement (based on a default threshold of 0.5) rather than a predicted probability. \n",
    "\n",
    "A related question would be: \"is the built-in function for roc_auc_score() receiving a predicted probability as it should?\" Coz if not, the cross-validated roc_auc_score and gini score printed above would be mis-leading as well! The following cell can confirm our suspicion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following cell to check the y_pred_prob which appear to be binary only\n",
    "scoring = {\n",
    "    'gini': make_scorer(lambda y_true, y_prob: (\n",
    "                            2*roc_auc_score(y_true, y_prob)-1,\n",
    "                            print(\"y_pred_prob values:\", y_prob)\n",
    "                        )),\n",
    "    'auc': make_scorer(lambda y_true, y_prob: (\n",
    "                            roc_auc_score(y_true, y_prob),\n",
    "                            print(\"y_pred_prob values:\", y_prob),\n",
    "                        ))\n",
    "}\n",
    "\n",
    "scores = cross_validate(model, X, y, cv=kfold, scoring=scoring)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1: Subclass and the revised `predict()`\n",
    "So now we have a problem to solve: We need to feed `make_scorer()` function with the predicted probability to get the correct cross_validated metrics.  \n",
    "\n",
    "There must be multiple ways to go around this. Below I will try two ways\n",
    "\n",
    "First, a sort of cheaky solution which poped into my head first is to create a new class (maybe due to or the OOP studies I did a while a go, see `OOP and multiple models.ipynb` also in this repo)\n",
    "* this new class is a subclass of LogisticRegression() and therefore inherit its abilities :P\n",
    "* then enforce this new class's `predict()` method to return the predicted probilities (rather than binary predictions). \n",
    "\n",
    "Below pls see the code to impletement this simple idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini scores: [0.98 0.99 0.96 0.99 0.98]\n",
      "AUC scores: [0.99 1.   0.98 0.99 0.99]\n",
      "f-beta: [0.98 0.98 0.95 0.96 0.95]\n",
      "Mean Gini (proba):           0.98\n",
      "Mean f-beta (proba):         0.96\n",
      "Mean AUC scores (proba):     0.99\n",
      "Mean AUC scores (original):  0.95\n"
     ]
    }
   ],
   "source": [
    "class proba_logreg(LogisticRegression):\n",
    "    def __init__(self):\n",
    "        super().__init__(max_iter=5000)\n",
    "    def predict(self, X):\n",
    "        return LogisticRegression.predict_proba(self, X)\n",
    "    \n",
    "model_proba = proba_logreg()\n",
    "\n",
    "def f_beta_prob(y_true, y_prob, threshold, beta):\n",
    "    \"\"\"\n",
    "    This function calculate Fbeta based on a threshold of 0.7, \n",
    "    which means cases with predicted probability higher than 0.7 \n",
    "    will be judged as positive by the model.\n",
    "    \"\"\"\n",
    "    y_pred = (y_prob[:,1] >= threshold).astype(int)\n",
    "    return fbeta_score(y_true, y_pred, beta = beta)\n",
    "\n",
    "# Update the scoring dictionary with the new custom metrics\n",
    "scoring = {\n",
    "    'gini': make_scorer(lambda y_true, y_prob: \n",
    "                            2*roc_auc_score(y_true, y_prob[:,1])-1\n",
    "                        ),\n",
    "    'auc': make_scorer(lambda y_true, y_prob:\n",
    "                            roc_auc_score(y_true, y_prob[:,1])\n",
    "                        ),\n",
    "    'f_beta': make_scorer(lambda y_true, y_prob:\n",
    "                            f_beta_prob(y_true, y_prob, threshold=0.7, beta=0.8)\n",
    "                        )\n",
    "}\n",
    "\n",
    "scores_proba = cross_validate(model_proba, X, y, cv=kfold, scoring=scoring)\n",
    "\n",
    "print(\"Gini scores:\", np.round(scores_proba['test_gini'],2))\n",
    "print(\"AUC scores:\", np.round(scores_proba['test_auc'],2))\n",
    "print(\"f-beta:\", np.round(scores_proba['test_f_beta'],2))\n",
    "print(\"Mean Gini (proba):           {:.2f}\".format(np.mean(scores_proba['test_gini'])))\n",
    "print(\"Mean f-beta (proba):         {:.2f}\".format(np.mean(scores_proba['test_f_beta'])))\n",
    "print(\"Mean AUC scores (proba):     {:.2f}\".format(np.mean(scores_proba['test_auc'])))\n",
    "print(\"Mean AUC scores (original):  {:.2f}\".format(np.mean(scores['test_auc'])))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: Do it manually\n",
    "\n",
    "Of course, we could simply do the splits and calculate the relevant metrics manually like below. \n",
    "\n",
    "The manual process comes with the benefit of ultimate customizability:\n",
    "\n",
    "1. it can be used to cross-validate any sklearn or customed algorithm class. (The first approach above is able to do this as well.) \n",
    "2. it can be easily applied from any stage of the modeling process. For example, if I have a customed class that does both feature engineering and model fit, using this approach I could either just test the fitted model acrossed kfold data or test the combined process by doing the kfold splits before feature engineering. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GINI: Mean = 0.99, Std Dev = 0.01\n",
      "AUC: Mean = 0.99, Std Dev = 0.00\n"
     ]
    }
   ],
   "source": [
    "# Set the number of folds\n",
    "k = 5\n",
    "\n",
    "# Split the data into k folds\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "# Initialize an array to store the results\n",
    "results = {'gini': np.zeros(k), 'auc': np.zeros(k)}\n",
    "\n",
    "# Loop over the folds\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    \n",
    "    # Get the training and testing data for this fold\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    model.fit(X_train, y_train)    \n",
    "    \n",
    "    # Evaluate the model on the testing data and store the results\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    results['gini'][i] = 2*roc_auc_score(y_test, y_prob) - 1\n",
    "    results['auc'][i] = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# Compute the mean and standard deviation of the results for each metric\n",
    "mean_results = {metric: np.mean(results[metric]) for metric in results}\n",
    "std_devs = {metric: np.std(results[metric]) for metric in results}\n",
    "\n",
    "# Print the results\n",
    "for metric in results:\n",
    "    print(\"{}: Mean = {:.2f}, Std Dev = {:.2f}\".format(metric.upper(), \n",
    "                                                        mean_results[metric], \n",
    "                                                        std_devs[metric]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
