{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In classication problems, we face the difficult tradeoff between *precision* and *recall*. One common way of balancing between the two is to optimize the F1 score, which is their harmonic mean. But we can imagine in some business problems, precision would be considered more important than recall, and vice versa. \n",
    "\n",
    "Here is where f_beta score comes in handy. We can adjust the value of beta to give higher weight to either precision or recall. Below please see a simple toy example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define two models, one with perfect precision, the other with perfect recall\n",
    "y_true           = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0] # half positive\n",
    "y_pred_precision = [1, 1, 1, 0, 0, 0, 0, 0, 0, 0] # perfect precision, two false negatives\n",
    "y_pred_recall    = [1, 1, 1, 1, 1, 1, 1, 1, 0, 0] # perfect recall, three false positives\n",
    "\n",
    "perfect_precision = [y_true, y_pred_precision]\n",
    "perfect_recall    = [y_true, y_pred_recall]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision 1.0\n",
      "recall 0.6\n"
     ]
    }
   ],
   "source": [
    "# the precision and recall of the model with perfect precision\n",
    "print('precision', precision_score(*perfect_precision))\n",
    "print('recall', recall_score(*perfect_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision 0.625\n",
      "recall 1.0\n"
     ]
    }
   ],
   "source": [
    "# the precision and recall of the model with perfect recall\n",
    "print('precision', precision_score(*perfect_recall))\n",
    "print('recall', recall_score(*perfect_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fbeta scores\n",
    "To summarize, the above two models has either perfect precision or recall, and scores about 0.6 on the other metric. \n",
    "\n",
    "If we use F1 to compete the two models, they will perform similarly, However, F0.5, which gives more weight to precision, will recommend pretty different choice from F2, which gives more weight to recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7499999999999999\n",
      "0.7692307692307693\n"
     ]
    }
   ],
   "source": [
    "# the two models score similarly when beta = 1\n",
    "print(fbeta_score(*perfect_precision, beta=1)) \n",
    "print(fbeta_score(*perfect_recall, beta=1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8823529411764706\n",
      "0.6756756756756757\n"
     ]
    }
   ],
   "source": [
    "# the model with perfect precision scores much higher when beta = 0.5\n",
    "print(fbeta_score(*perfect_precision, beta=0.5)) \n",
    "print(fbeta_score(*perfect_recall, beta=0.5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6521739130434783\n",
      "0.8928571428571429\n"
     ]
    }
   ],
   "source": [
    "# the model with perfect recall scores much higher when beta = 2\n",
    "print(fbeta_score(*perfect_precision, beta=2)) \n",
    "print(fbeta_score(*perfect_recall, beta=2)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38-modelling]",
   "language": "python",
   "name": "conda-env-py38-modelling-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
